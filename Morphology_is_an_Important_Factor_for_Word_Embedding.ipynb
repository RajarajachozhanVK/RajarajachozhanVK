{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfKzW485l7X9Aw+BedI+0/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajarajachozhanVK/RajarajachozhanVK/blob/main/Morphology_is_an_Important_Factor_for_Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Morphology is important factor for word Embedding**\n",
        "\n",
        "Aim: To develop algorithms for finding morphology of Word Documents.\n",
        "Procedure:\n",
        "Morphology is indeed an important factor for word embedding. Word embedding is a technique in\n",
        "natural language processing (NLP) that represents words as dense vectors in a continuous vector\n",
        "space. These embeddings capture semantic relationships and contextual information about words,\n",
        "enabling machines to better understand and process natural language.\n",
        "Morphology refers to the structure of words and how they are formed from smaller meaningful\n",
        "units called morphemes. Words can be broken down into morphemes, such as prefixes, suffixes,\n",
        "and roots, each contributing to the overall meaning of the word. Understanding morphology is\n",
        "crucial for accurate word representation in embedding models for several reasons:\n",
        "1. Semantic Similarity: Words with similar meanings often share morphological features.\n",
        "Embedding models that consider morphology can capture these relationships more effectively,\n",
        "improving the semantic similarity between words.\n",
        "2. Out-of-Vocabulary Handling: Morphological analysis helps in handling out-of-vocabulary\n",
        "words by recognizing common prefixes, suffixes, or roots. This allows the model to generalize\n",
        "to new or unseen words based on their morphological structure.\n",
        "3. Word Sense Disambiguation: Morphological information aids in distinguishing between\n",
        "different senses of a word. Words with the same spelling but different meanings (homographs)\n",
        "can be disambiguated based on their morphological context.\n",
        "4. Language Agglutination: In languages where words are often formed by adding prefixes,\n",
        "suffixes, or infixes, understanding morphology becomes crucial. Models that capture these\n",
        "morphological rules can generate more accurate embeddings.\n",
        "In this example, We will use the FastText library to create a word embedding model with morphol-\n",
        "ogy awareness. The FastText model inherently considers subword information, which is beneficial\n",
        "for capturing morphological nuances.\n",
        "1. The NLTK library is used for tokenization and stop-word removal.\n",
        "2. The FastText model is trained on the preprocessed data, considering word bigrams (word-\n",
        "Ngrams=2) to capture morphological information.\n",
        "3. The trained model is saved for later use.\n",
        "4. An example shows how to obtain the embedding for the word “morphology.”\n",
        "\n",
        "**Implementation**"
      ],
      "metadata": {
        "id": "wJ8EW18dqnSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDqlZo1trgFM",
        "outputId": "7b2c8cfe-3830-46b7-802e-5d7c026736b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLfCpRTzro6M",
        "outputId": "bdd6a069-d131-48b2-90fe-b6f84aff6791"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m859.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4239657 sha256=a73b679dc368636ffc3c136c28399cd2cda39f9d536443574ce32ad80a2b0814\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AKXczfvqfjP",
        "outputId": "4633f70b-cd2a-479d-87d1-ad26f23f5eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for \"morphology\": [-3.67912871e-05 -4.51257511e-04  3.27752699e-04  2.42517432e-04\n",
            " -7.41376027e-07 -6.91938272e-04 -3.17198079e-04  3.48138768e-04\n",
            "  1.95542147e-04 -5.99805266e-04 -2.36053820e-05 -2.24640913e-04\n",
            " -1.45704093e-04  1.25037594e-04  2.67465453e-04 -1.61467367e-04\n",
            " -3.69575457e-04 -1.26908606e-04  2.84750917e-04  7.62814525e-05\n",
            " -1.07449639e-04 -4.08452121e-04  3.95845156e-04 -1.16585805e-04\n",
            "  2.35396627e-04 -1.06461055e-04 -4.26862389e-05 -1.13754002e-04\n",
            "  2.43401955e-04  3.27853806e-04  9.78696335e-06  8.42085647e-05\n",
            "  3.66075808e-04  2.36371983e-04 -1.82176649e-04  2.96842860e-04\n",
            "  3.47433350e-04  9.30823517e-05  1.04221115e-04  5.78945655e-05\n",
            " -1.35618553e-04 -2.50680750e-04 -1.14625560e-04  2.65853800e-04\n",
            " -2.38906068e-05 -1.68257102e-04  3.32706579e-04  6.07833616e-04\n",
            " -3.30856041e-04  3.07992275e-04  4.57142996e-05  5.42950293e-04\n",
            " -4.90873994e-04 -7.48773236e-05 -4.81654279e-04  1.90033345e-04\n",
            "  3.66624910e-04 -3.54755204e-04 -1.10380883e-04  2.38397930e-04\n",
            "  6.55797021e-06  2.93019839e-04  2.85789254e-04  4.55610891e-04\n",
            "  4.20592580e-04  1.51647793e-04  3.29893053e-04 -4.07821644e-05\n",
            "  2.51184712e-04 -4.35665570e-04 -4.01428151e-05 -2.25268974e-04\n",
            "  2.77279731e-04  1.93321350e-04  1.82996286e-04 -1.58412586e-04\n",
            " -3.27500893e-04  1.76758957e-04 -1.67127539e-04  5.55317907e-04\n",
            " -1.35209659e-04 -2.56029336e-04 -5.30107936e-04 -2.31167229e-04\n",
            " -6.21172876e-05  3.32169118e-04  2.38575420e-04  2.12924948e-04\n",
            " -1.87494370e-04 -2.49467848e-04 -9.43415143e-05 -4.12176712e-04\n",
            "  3.53356620e-04  2.80453067e-04 -2.13366511e-04  2.54115730e-04\n",
            "  2.16565313e-04  2.79860251e-04  1.96203750e-04  1.72423861e-05]\n"
          ]
        }
      ],
      "source": [
        "#!pip install nltk\n",
        "#!pip install fasttext\n",
        "import nltk\n",
        "nltk.download('stopwords') # Download the stopwords corpus\n",
        "nltk.download('punkt')\n",
        "import fasttext\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "# Sample Word document data\n",
        "document_data = [\n",
        "    \"This is a sample document.\",\n",
        "    \"Word embeddings capture morphological information.\",\n",
        "    \"Morphology is essential for understanding words.\",\n",
        "    # Add more sentences as needed\n",
        "]\n",
        "# Preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(sentence):\n",
        "    tokens = word_tokenize(sentence.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words and token not in string.punctuation]\n",
        "    return \" \".join(tokens)\n",
        "preprocessed_data = [preprocess(sentence) for sentence in document_data]\n",
        "# Save the preprocessed data to a text file\n",
        "with open('morphology_document.txt', 'w', encoding='utf-8') as file:\n",
        "    for sentence in preprocessed_data:\n",
        "        file.write(sentence + '\\n')\n",
        "# Train FastText model with morphology awareness\n",
        "model = fasttext.train_unsupervised(\n",
        "    'morphology_document.txt',\n",
        "    model='skipgram',\n",
        "    wordNgrams=2,  # Consider word bigrams for morphological information\n",
        "    minCount=1,  # Minimum count of a word to be considered\n",
        "    epoch=10  # Number of training epochs\n",
        ")\n",
        "# Save the trained model\n",
        "model.save_model('morphology_word_embedding.bin')\n",
        "# Example: Get the embedding for a word\n",
        "word_embedding = model.get_word_vector('morphology')\n",
        "print(f'Embedding for \"morphology\": {word_embedding}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example:\n",
        "1. NLTK is used for tokenization, stop-word removal, and lemmatization.\n",
        "2. The Word2Vec model from gensim is trained on the preprocessed data.\n",
        "3. The trained model is saved for later use.\n",
        "4. An example shows how to obtain the embedding for the word “morphology.”"
      ],
      "metadata": {
        "id": "wcugUsufrRK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN7Ie5R6rSPT",
        "outputId": "b506863c-89f8-45f5-e304-fd816a26de45"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation"
      ],
      "metadata": {
        "id": "sbZat7L2tb-2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# Sample document data\n",
        "document_data = [\n",
        "    \"This is a sample document.\",\n",
        "    \"Word embeddings capture morphological information.\",\n",
        "    \"Morphology is essential for understanding words.\",\n",
        "    # Add more sentences as needed\n",
        "]\n",
        "# Preprocess the data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(sentence):\n",
        "    tokens = word_tokenize(sentence.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words and token not in punctuation]\n",
        "    return tokens\n",
        "preprocessed_data = [preprocess(sentence) for sentence in document_data]\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=preprocessed_data, vector_size=100, window=5, min_count=1, workers=4)\n",
        "# Save the trained model\n",
        "model.save(\"morphology_word_embedding.model\")\n",
        "# Example: Get the embedding for a word\n",
        "word_embedding = model.wv['morphology']\n",
        "print(f'Embedding for \"morphology\": {word_embedding}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3pS-hT7tepm",
        "outputId": "f06b3d1c-6cce-44a2-ef6f-92db2c37442e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for \"morphology\": [-8.2426779e-03  9.2993546e-03 -1.9766092e-04 -1.9672764e-03\n",
            "  4.6036304e-03 -4.0953159e-03  2.7431143e-03  6.9399667e-03\n",
            "  6.0654259e-03 -7.5107943e-03  9.3823504e-03  4.6718083e-03\n",
            "  3.9661205e-03 -6.2435055e-03  8.4599797e-03 -2.1501649e-03\n",
            "  8.8251876e-03 -5.3620026e-03 -8.1294188e-03  6.8245591e-03\n",
            "  1.6711927e-03 -2.1985089e-03  9.5136007e-03  9.4938548e-03\n",
            " -9.7740470e-03  2.5052286e-03  6.1566923e-03  3.8724565e-03\n",
            "  2.0227872e-03  4.3050171e-04  6.7363144e-04 -3.8206363e-03\n",
            " -7.1402504e-03 -2.0888723e-03  3.9238976e-03  8.8186832e-03\n",
            "  9.2591504e-03 -5.9759365e-03 -9.4026709e-03  9.7643770e-03\n",
            "  3.4297847e-03  5.1661171e-03  6.2823449e-03 -2.8042626e-03\n",
            "  7.3227035e-03  2.8302716e-03  2.8710044e-03 -2.3803699e-03\n",
            " -3.1282497e-03 -2.3701417e-03  4.2764368e-03  7.6057913e-05\n",
            " -9.5842788e-03 -9.6655441e-03 -6.1481940e-03 -1.2856961e-04\n",
            "  1.9974159e-03  9.4319675e-03  5.5843508e-03 -4.2906962e-03\n",
            "  2.7831673e-04  4.9643586e-03  7.6983096e-03 -1.1442233e-03\n",
            "  4.3234206e-03 -5.8143795e-03 -8.0419064e-04  8.1000505e-03\n",
            " -2.3600650e-03 -9.6634552e-03  5.7792603e-03 -3.9298222e-03\n",
            " -1.2228728e-03  9.9805174e-03 -2.2563506e-03 -4.7570644e-03\n",
            " -5.3293873e-03  6.9808899e-03 -5.7088719e-03  2.1136629e-03\n",
            " -5.2556600e-03  6.1207139e-03  4.3573068e-03  2.6063549e-03\n",
            " -1.4910829e-03 -2.7460635e-03  8.9929365e-03  5.2157748e-03\n",
            " -2.1625196e-03 -9.4703101e-03 -7.4260519e-03 -1.0637414e-03\n",
            " -7.9494715e-04 -2.5629092e-03  9.6827205e-03 -4.5852066e-04\n",
            "  5.8737611e-03 -7.4475873e-03 -2.5060738e-03 -5.5498634e-03]\n"
          ]
        }
      ]
    }
  ]
}