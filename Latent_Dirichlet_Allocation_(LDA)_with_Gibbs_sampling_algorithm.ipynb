{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZ2LC0yVSG/6xy1T7DOJ6g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajarajachozhanVK/RajarajachozhanVK/blob/main/Latent_Dirichlet_Allocation_(LDA)_with_Gibbs_sampling_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Latent Dirichlet Allocation (LDA) with Gibbs sampling algorithm ***"
      ],
      "metadata": {
        "id": "_d6KOh7a04Fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Introduction\n",
        "Latent Dirichlet Allocation (LDA) with Gibbs sampling is a popular probabilistic model used in\n",
        "natural language processing and machine learning for topic modeling.\n",
        "\n",
        "1.1 Notations and Definitions\n",
        "e Documents: D = dy,ds, ..., dy, where each d,,, represents a document.\n",
        "* Words in Documents: W = w,, ,, where w,y, ,,, denotes the n-th word in the m-th document.\n",
        "e Vocabulary: V= v;,vs,. .., vy, the set of all unique words across all documents.\n",
        "» Topics: K, the number of topics assumed in the corpus.\n",
        "« Topic Distribution: 6,,, the topic distribution for document d,, .\n",
        "» Topic Assignment: z,,, the topic assignment for the n-th word in document d,, .\n",
        "* Topic-Word Distribution: ¢, , the distribution of words for topic k.\n",
        "1.2 Model Assumptions\n",
        "« Dirichlet Priors: LDA assumes Dirichlet priors for topic distributions 6,,, and topic-word\n",
        "distributions ¢y\n",
        "* Exchangeability: Words are exchangeable within documents, meaning the order of words\n",
        "does not affect the underlying topic distribution.\n",
        "« 1.3 Generative Process LDA posits a generative process for each document d,, :\n",
        " For each document d,,, :\n",
        "o Draw topic distribution 8,,, ~ Dirichlet(c).\n",
        "o For each word Wy,\n",
        "= Draw topic assignment z{m,n} ~ Multinomial(6{m}).\n",
        "= Draw word w{m,n} from ¢{z,m}, the topic-word distribution for topic 2 , .\n",
        "1.5 Inference using Gibbs Sampling\n",
        "To infer the posterior distribution of latent variables Z, ©, and ®, particularly ® and ® for topic\n",
        "modeling with LDA, Gibbs sampling is often used.\n",
        "Gibbs Sampling Steps:\n",
        "1. Initialize ©, ®, and Z.\n",
        "2. Iterate through each word wyy, ,, in each document $d_{m}:\n",
        "2.1 Exclude wyy, , from © and ® and compute P(z,, , = k | otherZ, a, 8).\n",
        "2.2 Sample zp, ,, from the conditional distribution P2y, = k | otherZ, a, ).\n",
        "3. Update © and ® based on the new assignments of Z.\n",
        "4. Repeat until convergence or after a sufficient number of iterations.\n",
        "2. Procedure\n",
        "Step 1. Imports and Hyperparameters\n",
        "import numpy as np\n",
        "from scipy.special import gammaln\n",
        "# Hyperparameters\n",
        "alpha = 0.1 # Dirichlet parameter for topic distribution\n",
        "beta = 8.1 # Dirichlet parameter for word distribution\n",
        "* numpy is imported for numerical operations, and scipy.special.gammaln is imported for\n",
        "computing the logarithm of the gamma function.\n",
        "¢ alpha and beta are Dirichlet priors for the topic distributions and word distributions,\n",
        "respectively."
      ],
      "metadata": {
        "id": "PTWdH77K066E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gGwx21jE0D0i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.special import gammaln\n",
        "import random\n",
        "from collections import defaultdict\n",
        "# Ensure reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "# Hyperparameters\n",
        "alpha = 0.1  # Dirichlet parameter for topic distribution\n",
        "beta = 0.1   # Dirichlet parameter for word distribution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example documents\n",
        "documents = [\n",
        "    ['this', 'is', 'the', 'first', 'document'],\n",
        "    ['this', 'is', 'the', 'second', 'document'],\n",
        "    ['and', 'this', 'is', 'the', 'third', 'one'],\n",
        "    ['is', 'this', 'a', 'test'],\n",
        "    ['this', 'document', 'is', 'a', 'test']\n",
        "]\n",
        "# Create vocabulary and mappings\n",
        "vocab = list(set(word for doc in documents for word in doc))\n",
        "vocab_size = len(vocab)\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "index_to_word = {i: word for word, i in word_to_index.items()}\n",
        "# Convert documents to word indices\n",
        "docs_ids = [[word_to_index[word] for word in doc] for doc in documents]"
      ],
      "metadata": {
        "id": "HMof7i2-1OZp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_topics = 2\n",
        "n_docs = len(documents)\n",
        "doc_lengths = np.array([len(doc) for doc in documents])\n",
        "# Initialize topic assignments randomly\n",
        "topic_assignments = [[random.randint(0, n_topics - 1) for _ in doc] for doc in documents]\n",
        "# Initialize count matrices\n",
        "doc_topic_counts = np.zeros((n_docs, n_topics))  # Number of words assigned to each topic in each document\n",
        "topic_word_counts = np.zeros((n_topics, vocab_size))  # Number of times each word is assigned to each topic\n",
        "topic_counts = np.zeros(n_topics)  # Total number of words assigned to each topic\n",
        "# Populate the count matrices\n",
        "for d, doc in enumerate(docs_ids):\n",
        "    for i, word in enumerate(doc):\n",
        "        topic = topic_assignments[d][i]\n",
        "        doc_topic_counts[d][topic] += 1\n",
        "        topic_word_counts[topic][word] += 1\n",
        "        topic_counts[topic] += 1"
      ],
      "metadata": {
        "id": "me4SvDdo1SM2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 1000\n",
        "for _ in range(num_samples):\n",
        "    for d, doc in enumerate(docs_ids):\n",
        "        for i, word in enumerate(doc):\n",
        "            current_topic = topic_assignments[d][i]\n",
        "            # Decrement counts for the current word and topic\n",
        "            doc_topic_counts[d][current_topic] -= 1\n",
        "            topic_word_counts[current_topic][word] -= 1\n",
        "            topic_counts[current_topic] -= 1\n",
        "            # Calculate topic probabilities\n",
        "            topic_probs = (doc_topic_counts[d] + alpha) * (topic_word_counts[:, word] + beta) / (topic_counts + vocab_size * beta)\n",
        "            topic_probs /= np.sum(topic_probs)\n",
        "            # Sample a new topic based on the probabilities\n",
        "            new_topic = np.random.choice(np.arange(n_topics), p=topic_probs)\n",
        "            topic_assignments[d][i] = new_topic\n",
        "            # Increment counts for the new word and topic\n",
        "            doc_topic_counts[d][new_topic] += 1\n",
        "            topic_word_counts[new_topic][word] += 1\n",
        "            topic_counts[new_topic] += 1"
      ],
      "metadata": {
        "id": "2bBu7Tb31WNX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute topic proportions for each document\n",
        "doc_topic_probs = (doc_topic_counts + alpha) / (doc_lengths[:, None] + n_topics * alpha)"
      ],
      "metadata": {
        "id": "BWAOhhSC1b42"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print topic proportions for each document\n",
        "for d, doc in enumerate(documents):\n",
        "    print(f\"Document {d+1}: {doc}\")\n",
        "    for topic in range(n_topics):\n",
        "        print(f\"  Topic {topic}: {doc_topic_probs[d][topic]:.3f}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2lCv-6u1eUi",
        "outputId": "952b67b4-8cc5-49a1-a103-00e7a346d0ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1: ['this', 'is', 'the', 'first', 'document']\n",
            "  Topic 0: 0.981\n",
            "  Topic 1: 0.019\n",
            "\n",
            "Document 2: ['this', 'is', 'the', 'second', 'document']\n",
            "  Topic 0: 0.981\n",
            "  Topic 1: 0.019\n",
            "\n",
            "Document 3: ['and', 'this', 'is', 'the', 'third', 'one']\n",
            "  Topic 0: 0.500\n",
            "  Topic 1: 0.500\n",
            "\n",
            "Document 4: ['is', 'this', 'a', 'test']\n",
            "  Topic 0: 0.976\n",
            "  Topic 1: 0.024\n",
            "\n",
            "Document 5: ['this', 'document', 'is', 'a', 'test']\n",
            "  Topic 0: 0.981\n",
            "  Topic 1: 0.019\n",
            "\n"
          ]
        }
      ]
    }
  ]
}