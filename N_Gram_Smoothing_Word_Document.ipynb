{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLLRV+9SYxbi43SUdZWcTf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajarajachozhanVK/RajarajachozhanVK/blob/main/N_Gram_Smoothing_Word_Document.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-Grams_Smoothing-Word Documents\n",
        "**bold text**\n",
        "\n",
        "Aim: To perform N-Grams Smoothing in Word Documents\n",
        "1. N-Grams Smoothing N-gram smoothing is a technique used in natural language processing\n",
        "and language modeling to address the problem of unseen n-grams. An n-gram is a contiguous\n",
        "sequence of n items (words, characters, or symbols) within a given text. Smoothing is applied to\n",
        "handle cases where some n-grams have zero probability in the training data, leading to issues when\n",
        "estimating probabilities for unseen n-grams in real-world text.\n",
        "There are many ways to do smoothing, and some of them are:\n",
        "• Laplace (add one) smoothing\n",
        "• Add-k smoothing\n",
        "• Stupid backoff\n",
        "• Kneser-Ney smoothing\n",
        "2. Laplace (Add 1) Smoothing The simplest way to do smoothing is to add one to all the\n",
        "n-gram counts, before we normalize them into probabilities. All the counts that used to be zero\n",
        "will now have a count of 1, the counts of 1 will be 2, and so on. This algorithm is called Laplace\n",
        "smoothing.\n",
        "The formula for add-one smoothing is as follows:\n",
        "𝑃𝐿𝑎𝑝𝑙𝑎𝑐𝑒𝑆𝑚𝑜𝑜𝑡ℎ𝑖𝑛𝑔(𝑤𝑛 ⁄ 𝑤(𝑛−1), …, 𝑤1\n",
        ") = (𝐶(𝑤(𝑛−1), …, 𝑤1\n",
        ", 𝑤𝑛) + 1) ⁄ (𝐶(𝑤(𝑛−1), …, 𝑤1\n",
        ") + 𝑉 ) =\n",
        "(𝐶(𝑤(𝑛−1)𝑤𝑛) + 1) ⁄ (𝐶(𝑤(𝑛−1)) + 𝑉 )\n",
        "• 𝐶(𝑤(𝑛−1), …, 𝑤1\n",
        ", 𝑤𝑛) is the count of the n-gram in the training data.\n",
        "• 𝐶(𝑤(𝑛−1), …, 𝑤1\n",
        ") is the count of the (n-1)-gram prefix in the training data.\n",
        "• V is the vocabulary size (the number of unique words in the training data).\n",
        "This formula ensures that the probability distribution is smoothed, allowing for some probability\n",
        "mass to be distributed to unseen n-grams.\n",
        "Performing N-grams smoothing in word documents typically involves applying a statistical language\n",
        "modeling technique to adjust the probabilities of n-grams (sequences of n words) to account for\n",
        "unseen or infrequently occurring combinations of words. Here are the general steps to perform\n",
        "N-grams smoothing in word documents.\n",
        "\n",
        "Steps to Apply Laplace (Add 1) Smoothing Algorithm:\n",
        "1. Tokenization: Break the text into words or tokens. You can use various tokenization libraries\n",
        "or functions available in programming languages such as Python.\n",
        "1\n",
        "2. N-grams Generation: Generate n-grams (sequences of n words) from the tokenized text.\n",
        "Common choices are unigrams (1-grams), bigrams (2-grams), trigrams (3-grams), etc.\n",
        "3. Counting N-grams: Count the occurrences of each n-gram in the document. This involves\n",
        "creating a frequency distribution of n-grams.\n",
        "4. Smoothing Technique: Apply the chosen smoothing technique to handle unseen n-grams.\n",
        "Common smoothing techniques include Laplace (add-one) smoothing, Lidstone smoothing,\n",
        "and Good-Turing smoothing.\n",
        "5. Calculate Probabilities: Calculate the probabilities of each n-gram using the chosen smoothing\n",
        "technique. This step involves adjusting the counts to handle unseen n-grams and prevent zero\n",
        "probabilities.\n",
        "6. Apply Smoothing: Apply the calculated probabilities to your language model. This adjusted\n",
        "model can now provide more robust estimates of word sequences.\n",
        "4(A) Laplace (Add 1) Smoothing Implementation Laplace smoothing adds a count of 1\n",
        "to each event’s frequency to ensure that no probability is zero. This is particularly useful when\n",
        "dealing with categorical data where some categories might not appear in the training data"
      ],
      "metadata": {
        "id": "leng1207PMyG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XuuB9A1NgDh",
        "outputId": "0d2b9502-52e4-49ee-8f71-e2b5e521f29e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smoothed probabilities: [0.4, 0.3, 0.1, 0.2]\n"
          ]
        }
      ],
      "source": [
        "def laplace_smoothing(counts, vocab_size):\n",
        "    # Calculate the total count of all events\n",
        "    total_count = sum(counts)\n",
        "    # Apply the Laplace smoothing formula to each count\n",
        "    smoothed_probs = [(count + 1) / (total_count + vocab_size) for count in counts]\n",
        "    return smoothed_probs\n",
        "# Example usage:\n",
        "# Define the counts of events (e.g., word frequencies in a corpus)\n",
        "counts = [3, 2, 0, 1]  # counts for \"cat\", \"dog\", \"fish\", \"bird\"\n",
        "vocab_size = 4  # size of the vocabulary (number of unique words)\n",
        "# Calculate the smoothed probabilities\n",
        "smoothed_probs = laplace_smoothing(counts, vocab_size)\n",
        "# Print the smoothed probabilities\n",
        "print(\"Smoothed probabilities:\", smoothed_probs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from collections import Counter\n",
        "from nltk import ngrams, word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2scTZPFsUwKY",
        "outputId": "62424e57-bc2d-47cb-950d-6137d6a957ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "    return word_tokenize(text.lower())\n",
        "def generate_ngrams(tokens, n):\n",
        "    return list(ngrams(tokens, n))\n",
        "def add_one_smoothing(ngram_counts, vocabulary_size):\n",
        "    smoothed_probs = {}\n",
        "    prefix_counts = Counter()\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        prefix = ngram[:-1]\n",
        "        prefix_counts[prefix] += count\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        prefix = ngram[:-1]\n",
        "        prefix_count = prefix_counts[prefix] if prefix in prefix_counts else 0\n",
        "        smoothed_probs[ngram] = (count + 1) / (prefix_count + vocabulary_size)\n",
        "    return smoothed_probs\n",
        "def main():\n",
        "    # Example text\n",
        "    document = \"This is an example document. It contains words for n-gram smoothing.\"\n",
        "    # Tokenization\n",
        "    tokens = tokenize_text(document)\n",
        "    # N-gram generation (using bigrams as an example)\n",
        "    bigrams = generate_ngrams(tokens, 2)\n",
        "    # Counting n-gram occurrences\n",
        "    ngram_counts = Counter(bigrams)\n",
        "    # Vocabulary size\n",
        "    vocabulary_size = len(set(tokens))\n",
        "    # Add-one smoothing\n",
        "    smoothed_probs = add_one_smoothing(ngram_counts, vocabulary_size)\n",
        "    # Print the original and smoothed probabilities\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        print(f\"Original Probability of {ngram}: {count / len(tokens)}\")\n",
        "        print(f\"Smoothed Probability of {ngram}: {smoothed_probs[ngram]}\")\n",
        "        print()\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "5F4cuGKFP5DT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddc8f769-9e55-478d-9b12-229c7e57a214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Probability of ('this', 'is'): 0.07692307692307693\n",
            "Smoothed Probability of ('this', 'is'): 0.15384615384615385\n",
            "\n",
            "Original Probability of ('is', 'an'): 0.07692307692307693\n",
            "Smoothed Probability of ('is', 'an'): 0.15384615384615385\n",
            "\n",
            "Original Probability of ('an', 'example'): 0.07692307692307693\n",
            "Smoothed Probability of ('an', 'example'): 0.15384615384615385\n",
            "\n",
            "Original Probability of ('example', 'document'): 0.07692307692307693\n",
            "Smoothed Probability of ('example', 'document'): 0.15384615384615385\n",
            "\n",
            "Original Probability of ('document', '.'): 0.07692307692307693\n",
            "Smoothed Probability of ('document', '.'): 0.15384615384615385\n",
            "\n",
            "Original Probability of ('.', 'it'): 0.07692307692307693\n",
            "Smoothed Probability of ('.', 'it'): 0.15384615384615385\n",
            "\n",
            "Original Probability of ('it', 'contains'): 0.07692307692307693\n",
            "Smoothed Probability of ('it', 'contains'): 0.15384615384615385\n",
            "\n",
            "Original Probability of ('contains', 'words'): 0.07692307692307693\n",
            "Smoothed Probability of ('contains', 'words'): 0.15384615384615385\n",
            "\n",
            "Original Probability of ('words', 'for'): 0.07692307692307693\n",
            "Smoothed Probability of ('words', 'for'): 0.15384615384615385\n",
            "\n",
            "Original Probability of ('for', 'n-gram'): 0.07692307692307693\n",
            "Smoothed Probability of ('for', 'n-gram'): 0.15384615384615385\n",
            "\n",
            "Original Probability of ('n-gram', 'smoothing'): 0.07692307692307693\n",
            "Smoothed Probability of ('n-gram', 'smoothing'): 0.15384615384615385\n",
            "\n",
            "Original Probability of ('smoothing', '.'): 0.07692307692307693\n",
            "Smoothed Probability of ('smoothing', '.'): 0.15384615384615385\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Add-k Smoothing Add-k smoothing, also known as Laplace smoothing, is a technique\n",
        "used in probability and statistics to handle the issue of zero probabilities in categorical data,\n",
        "especially in the context of natural language processing and Bayesian models. It ensures that every\n",
        "possible outcome has a non-zero probability, which is particularly useful in applications like text\n",
        "classification, language modeling, and spam detection.\n",
        "Here’s a step-by-step explanation of how add-k smoothing works and how you can apply it:\n",
        "Basic Idea Add-k smoothing modifies the probability estimates by adding a small constant k to\n",
        "each count. This prevents any probability from being zero and distributes some probability mass\n",
        "to unseen events.\n",
        "Formula Given:\n",
        "\n",
        "Steps to Apply Add k Smoothing Algorithm:\n",
        "1. Count the Events: Calculate the\n",
        "frequency of each event in your data.\n",
        "2. Apply the Smoothing Formula: Adjust each count\n",
        "by adding k and normalize by the total counts plus the smoothing term.\n",
        "Example Consider a simple example with a vocabulary of size 4 (words: “cat”, “dog”, “fish”,\n",
        "“bird”) and observed counts:\n",
        "\n",
        "• “cat”: 3\n",
        "\n",
        "• “dog”: 2\n",
        "\n",
        "• “fish”: 0\n",
        "\n",
        "• “bird”: 1\n",
        "\n",
        "The total number of observations ff is 3 + 2 + 0 + 1 = 6\n",
        "With add-1 smoothing (i.e., K = 1):\n",
        "\n",
        "P(cat) = (3+1)/(6+4*1) = 4/10 = 0.4\n",
        "\n",
        "P(dog) = (2+1)/(6+4*1) = 3/10 = 0.3\n",
        "\n",
        "P(fish) = (0+1)/(6+4*1) = 1/10 = 0.1\n",
        "\n",
        "P(bird) = (1+1)/(6+4*1) = 2/10 = 0.2"
      ],
      "metadata": {
        "id": "ZfcwplQxQG80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_k_smoothing(counts, k, vocab_size):\n",
        "    \"\"\"\n",
        "    Apply add-k smoothing to a list of counts.\n",
        "\n",
        "    Args:\n",
        "    counts (list of int): The observed counts of each event.\n",
        "    k (float): The smoothing parameter.\n",
        "    vocab_size (int): The number of unique events (vocabulary size).\n",
        "\n",
        "    Returns:\n",
        "    list of float: The smoothed probabilities.\n",
        "    \"\"\"\n",
        "    # Calculate the total count of all events\n",
        "    total_count = sum(counts)\n",
        "    # Apply the add-k smoothing formula to each count\n",
        "    smoothed_probs = [(count + k) / (total_count + k * vocab_size) for count in counts]\n",
        "    return smoothed_probs\n",
        "# Example usage:\n",
        "# Define the counts of events (e.g., word frequencies in a corpus)\n",
        "counts = [3, 2, 0, 1]  # counts for \"cat\", \"dog\", \"fish\", \"bird\"\n",
        "k = 0.5  # smoothing parameter (Laplace smoothing)\n",
        "vocab_size = 4  # size of the vocabulary (number of unique words)\n",
        "# Calculate the smoothed probabilities\n",
        "smoothed_probs = add_k_smoothing(counts, k, vocab_size)\n",
        "# Print the smoothed probabilities\n",
        "print(\"Smoothed probabilities:\", smoothed_probs)"
      ],
      "metadata": {
        "id": "TiSSyQmOQYdk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed515ad8-85d0-42ff-b4fd-724926bef25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smoothed probabilities: [0.4375, 0.3125, 0.0625, 0.1875]\n"
          ]
        }
      ]
    }
  ]
}