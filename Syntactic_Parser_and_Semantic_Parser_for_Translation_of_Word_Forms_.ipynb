{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoFwamGnbzmqGrE8BIWLdJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajarajachozhanVK/RajarajachozhanVK/blob/main/Syntactic_Parser_and_Semantic_Parser_for_Translation_of_Word_Forms_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syntactic Parser and Semantic Parser for Translation of Word Forms\n",
        "\n",
        "Introduction\n",
        "\n",
        "The aim is to develop a syntactic and semantic parser to translate word forms. In this context, we will focus on syntactic parsing to identify and structure parts of speech and semantic parsing to understand and derive meaning from the text.\n",
        "\n",
        "Syntactic Parsing\n",
        "Syntactic parsing involves analyzing the grammatical structure of a sentence to understand the relationships between words. It identifies parts of speech (POS) and arranges words into a parse tree.\n",
        "\n",
        "Tokenization: Break text into tokens (words or phrases).\n",
        "POS Tagging: Assign grammatical categories (tags) to each token (e.g., noun, verb).\n",
        "Chunking: Group tokens into meaningful phrases (e.g., noun phrases, verb phrases).\n",
        "Semantic Parsing\n",
        "Semantic parsing involves understanding the meaning of the text by interpreting the relationships and roles of words within a sentence.\n",
        "\n",
        "Named Entity Recognition (NER): Identify and classify entities (e.g., names, dates, locations).\n",
        "Relation Extraction: Determine the relationships between entities.\n",
        "Semantic Role Labeling: Assign roles to words (e.g., agent, object)."
      ],
      "metadata": {
        "id": "yrKGy7P6zFQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Syntactic Parser Implementation**"
      ],
      "metadata": {
        "id": "5egwCRyvy1dJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjUxucpZxfkq",
        "outputId": "e0172825-25b8-4cfb-9feb-f9d62999b9ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This domain\n",
            "prior coordination\n",
            "permission\n",
            "information\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "# Ensure you have the necessary NLTK resources downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Tokenize the text into words\n",
        "def tokenize_and_tag(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    return tagged_tokens\n",
        "\n",
        "# Define your chunking pattern\n",
        "def chunk_text(tagged_tokens):\n",
        "    pattern = 'NP: {<DT>?<JJ>*<NN>}'  # Chunk noun phrases\n",
        "    cp = RegexpParser(pattern)  # Create a chunk parser\n",
        "    chunked_tree = cp.parse(tagged_tokens)\n",
        "    return chunked_tree\n",
        "\n",
        "# Extract chunks based on the specified pattern\n",
        "def extract_chunks(chunked_tree):\n",
        "    chunks = []\n",
        "    for subtree in chunked_tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
        "        chunks.append(' '.join(word for word, pos in subtree.leaves()))\n",
        "    return chunks\n",
        "\n",
        "# Example usage\n",
        "text = \"This domain uses prior coordination and permission information.\"\n",
        "tagged_tokens = tokenize_and_tag(text)\n",
        "chunked_tree = chunk_text(tagged_tokens)\n",
        "chunks = extract_chunks(chunked_tree)\n",
        "\n",
        "# Print chunked sentences\n",
        "for chunk in chunks:\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Semantic Parser Implementation**"
      ],
      "metadata": {
        "id": "A2S5bwdvyzG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to perform named entity recognition\n",
        "def named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# Function to perform relation extraction\n",
        "def relation_extraction(text):\n",
        "    doc = nlp(text)\n",
        "    relations = []\n",
        "    for ent in doc.ents:\n",
        "        for token in ent.root.head.children:\n",
        "            if token.dep_ in ('attr', 'dobj'):\n",
        "                relations.append((ent.text, token.head.text, token.text))\n",
        "    return relations\n",
        "\n",
        "# Example usage\n",
        "text = \"Barack Obama was born in Hawaii.\"\n",
        "entities = named_entity_recognition(text)\n",
        "relations = relation_extraction(text)\n",
        "\n",
        "# Print named entities\n",
        "print(\"Named Entities:\")\n",
        "for entity in entities:\n",
        "    print(entity)\n",
        "\n",
        "# Print relations\n",
        "print(\"Relations:\")\n",
        "for relation in relations:\n",
        "    print(relation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR-OtlZyyuZ3",
        "outputId": "1be1cf6d-afbb-47f3-f091-2fdf97b7d2fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities:\n",
            "('Barack Obama', 'PERSON')\n",
            "('Hawaii', 'GPE')\n",
            "Relations:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "# Ensure you have the necessary NLTK resources downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Fetches content from a URL and parses the text\n",
        "def fetch_html_content(url):\n",
        "    response = requests.get(url)\n",
        "    return response.text\n",
        "\n",
        "def parse_html_to_text(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    text = soup.get_text()\n",
        "    return text\n",
        "\n",
        "# Tokenize the text into words\n",
        "def tokenize_and_tag(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    return tagged_tokens\n",
        "\n",
        "# Define your chunking pattern\n",
        "def chunk_text(tagged_tokens):\n",
        "    pattern = 'NP: {<DT>?<JJ>*<NN>}'  # Chunk noun phrases\n",
        "    cp = RegexpParser(pattern)  # Create a chunk parser\n",
        "    chunked_tree = cp.parse(tagged_tokens)\n",
        "    return chunked_tree\n",
        "\n",
        "# Extract chunks based on the specified pattern\n",
        "def extract_chunks(chunked_tree):\n",
        "    chunks = []\n",
        "    for subtree in chunked_tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
        "        chunks.append(' '.join(word for word, pos in subtree.leaves()))\n",
        "    return chunks\n",
        "\n",
        "# Example usage\n",
        "url = 'http://example.com'  # Replace with your target URL\n",
        "html_content = fetch_html_content(url)\n",
        "text = parse_html_to_text(html_content)\n",
        "tagged_tokens = tokenize_and_tag(text)\n",
        "chunked_tree = chunk_text(tagged_tokens)\n",
        "chunks = extract_chunks(chunked_tree)\n",
        "\n",
        "# Print chunked sentences\n",
        "for chunk in chunks:\n",
        "    print(chunk)\n",
        "\n",
        "# Print chunked subtrees\n",
        "for subtree in chunked_tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
        "    print(subtree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nszXV8jSzoPn",
        "outputId": "cb6167f0-589a-467f-efb5-621779e06fd6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This domain\n",
            "use\n",
            "this domain\n",
            "literature\n",
            "prior coordination\n",
            "permission\n",
            "information\n",
            "(NP This/DT domain/NN)\n",
            "(NP use/NN)\n",
            "(NP this/DT domain/NN)\n",
            "(NP literature/NN)\n",
            "(NP prior/JJ coordination/NN)\n",
            "(NP permission/NN)\n",
            "(NP information/NN)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    }
  ]
}