{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajarajachozhanVK/RajarajachozhanVK/blob/main/Text_Preprocessing_Techniques_with_NLTK_and_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Tokenization**"
      ],
      "metadata": {
        "id": "b_UwlsPsIPo7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR-_jv0lHYAr"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "M4OtIHXEIoNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xTmVTGNJtbS",
        "outputId": "f80b31c0-3d60-408a-ceb2-67816903188d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "# Input text\n",
        "doc = nlp(\"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (Natural) Language.\")\n",
        "for tokens in doc:\n",
        "    print(\"Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXqeSdWPIz6L",
        "outputId": "b85fd97e-402a-450f-9e0c-729ac40ef17f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: Natural\n",
            "Tokens: language\n",
            "Tokens: processing\n",
            "Tokens: is\n",
            "Tokens: a\n",
            "Tokens: field\n",
            "Tokens: of\n",
            "Tokens: artificial\n",
            "Tokens: intelligence\n",
            "Tokens: that\n",
            "Tokens: deals\n",
            "Tokens: with\n",
            "Tokens: the\n",
            "Tokens: interaction\n",
            "Tokens: between\n",
            "Tokens: computers\n",
            "Tokens: and\n",
            "Tokens: human\n",
            "Tokens: (\n",
            "Tokens: Natural\n",
            "Tokens: )\n",
            "Tokens: Language\n",
            "Tokens: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxlyGY3HI4kb",
        "outputId": "70f7dd4c-57b5-4d59-9642-66dec53f0263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple PROPN nsubj\n",
            "is AUX aux\n",
            "looking VERB ROOT\n",
            "at ADP prep\n",
            "buying VERB pcomp\n",
            "U.K. PROPN dobj\n",
            "startup NOUN dep\n",
            "for ADP prep\n",
            "$ SYM quantmod\n",
            "1 NUM compound\n",
            "billion NUM pobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (Natural) Language.\"\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsgVEN0kJLar",
        "outputId": "956d1b59-61e8-4f41-8c4b-8a5e828c002c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'deals', 'with', 'the', 'interaction', 'between', 'computers', 'and', 'human', '(', 'Natural', ')', 'Language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Lowercasing**"
      ],
      "metadata": {
        "id": "uiH-u0BsJAlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (Natural) Language.\"\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "# lowercase the tokens\n",
        "lowercased_tokens = [token.lower() for token in tokens]\n",
        "print(\"Lowercased tokens:\", lowercased_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4vVRKcUJChi",
        "outputId": "c30a6a52-5ef5-40d0-e1a9-df6bdcedea30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercased tokens: ['natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'deals', 'with', 'the', 'interaction', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Remove punctuation**"
      ],
      "metadata": {
        "id": "Rde9ykt4Jz5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "Rj5h6bzqJ2Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuations(text):\n",
        "    # Tokenize the text into individual words\n",
        "    words = nltk.word_tokenize(text)\n",
        "    # Remove punctuations from each word\n",
        "    words_without_punctuations = [word for word in words if word not in string.punctuation]\n",
        "    # Join the words back into a string\n",
        "    text_without_punctuations = \" \".join(words_without_punctuations)\n",
        "    return text_without_punctuations"
      ],
      "metadata": {
        "id": "-NHF-MuCJ7Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function\n",
        "input_text = input(\"Enter a string: \")\n",
        "text_without_punctuations = remove_punctuations(input_text)\n",
        "print(\"String without punctuations:\", text_without_punctuations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oanarKmxJ_Wj",
        "outputId": "bb75869f-b515-47d9-ab68-ebf63d57b1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a string: Hello World!! @ Vel New3$\n",
            "String without punctuations: Hello World Vel New3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Remove stop words**"
      ],
      "metadata": {
        "id": "0aXUjaIDKJ6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "def remove_stop_words(string):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = string.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    new_string = ' '.join(filtered_words)\n",
        "    return new_string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYqk3HeLKLBL",
        "outputId": "d9de75a3-a905-4433-efcd-3eec03138dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_string = \"This is an example sentence to remove stop words from.\"\n",
        "result = remove_stop_words(input_string)\n",
        "print(\"Original string:\", input_string)\n",
        "print(\"Modified string:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATURE0NfKPgD",
        "outputId": "aed39df5-25c2-4e20-d79a-f484b556c43e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original string: This is an example sentence to remove stop words from.\n",
            "Modified string: example sentence remove stop words from.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(string):\n",
        "    # Load the spaCy English language model\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    # Tokenize the string into individual words\n",
        "    doc = nlp(string)\n",
        "    # Filter out stop words\n",
        "    filtered_words = [token.text for token in doc if not token.is_stop]\n",
        "    # Join the filtered words back into a string\n",
        "    new_string = ' '.join(filtered_words)\n",
        "    return new_string"
      ],
      "metadata": {
        "id": "0xzW0bS-KSei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_string = \"This is an example sentence to remove stop words from.\"\n",
        "result = remove_stop_words(input_string)\n",
        "print(\"Original string:\", input_string)\n",
        "print(\"Modified string:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLf0IaDDKZGi",
        "outputId": "d8fe6a47-5418-4e1f-ffb7-cc6b45f185ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original string: This is an example sentence to remove stop words from.\n",
            "Modified string: example sentence remove stop words .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Remove extra whitespace**"
      ],
      "metadata": {
        "id": "FEwgSZ6oKhXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input text with extra white space\n",
        "text = \"  Natural   language processing   is   a field   of artificial intelligence   that deals with the interaction between computers and human   (natural)   language.   \"\n",
        "# remove leading and trailing white space\n",
        "text = text.strip()\n",
        "# replace multiple consecutive white space characters with a single space\n",
        "text = \" \".join(text.split())\n",
        "print(\"Cleaned text:\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--Ps-08WKkWy",
        "outputId": "af1c8951-1804-4c32-cc19-138dbc93af79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned text: Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Remove URLs**"
      ],
      "metadata": {
        "id": "3Bf8MLNXKojz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "RsPpWoS5KrKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input text with URLs\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language. Check out this article for more information: https://www.veltech.edu.in/\"\n",
        "# define a regular expression pattern to match URLs\n",
        "pattern = r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\"\n",
        "# replace URLs with an empty string\n",
        "cleaned_text = re.sub(pattern, \"\", text)\n",
        "print(\"Text without URLs:\", cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-_HB4aZKuIi",
        "outputId": "47765cf5-9a5f-42f7-fa2c-b646f34ed1e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without URLs: Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language. Check out this article for more information: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Remove HTML code**"
      ],
      "metadata": {
        "id": "IWwS1HxnKztT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input text with HTML code\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language. <b>This is an example of bold text.</b>\"\n",
        "# define a regular expression pattern to match HTML tags\n",
        "pattern = r\"<[^>]+>\"\n",
        "# replace HTML tags with an empty string\n",
        "cleaned_text = re.sub(pattern, \"\", text)\n",
        "print(\"Text without HTML code:\", cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npAdQjlxK0vD",
        "outputId": "9cd94389-de62-4341-9712-a4ef1f2ea646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without HTML code: Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language. This is an example of bold text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Remove frequent words**"
      ],
      "metadata": {
        "id": "xP4WFOdZK4ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "# calculate the frequency of each word\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "# remove the most common words (e.g., the top 10% of words by frequency)\n",
        "filtered_tokens = [token for token in tokens if fdist[token] < fdist.N() * 0.1]\n",
        "print(\"Tokens without frequent words:\", filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5Enl-4KK8hj",
        "outputId": "d9d66119-f081-40e5-bec8-aa1340066dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens without frequent words: ['Natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'deals', 'with', 'the', 'interaction', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Spelling correction**"
      ],
      "metadata": {
        "id": "7NiwROVNK-77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17owzSMcLBXT",
        "outputId": "718390d6-9ccd-4779-f21f-30c09fea91d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural langage processing is a field of artificial intelligece that deals with the interaction between computers and human (naturl) langage.\"\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "# get list of English words\n",
        "words = nltk.corpus.words.words()\n",
        "# correct spelling of each word\n",
        "corrected_tokens = []\n",
        "for token in tokens:\n",
        "    # find the word with the lowest edit distance\n",
        "    corrected_token = min(words, key=lambda x: nltk.edit_distance(x, token))\n",
        "    corrected_tokens.append(corrected_token)\n",
        "print(\"Corrected tokens:\", corrected_tokens)"
      ],
      "metadata": {
        "id": "udEQc0UPLOeZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "74dea3b9-fcb5-4f6a-ba55-a11667312f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-28c49eb2f78b>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# find the word with the lowest edit distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcorrected_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medit_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcorrected_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrected_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Corrected tokens:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrected_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-28c49eb2f78b>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# find the word with the lowest edit distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcorrected_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medit_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcorrected_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrected_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Corrected tokens:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrected_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/metrics/distance.py\u001b[0m in \u001b[0;36medit_distance\u001b[0;34m(s1, s2, substitution_cost, transpositions)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mlast_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_left_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mlast_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_right_buf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0mlast_right_buf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             _edit_dist_step(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Stemming**"
      ],
      "metadata": {
        "id": "7Fz4owFBLVKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "# create stemmer object\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "# stem each token\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "print(\"Stemmed tokens:\", stemmed_tokens)"
      ],
      "metadata": {
        "id": "ukn2HJeKLce0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ab8ade-1740-4d95-e2c6-67c8211df79a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed tokens: ['natur', 'languag', 'process', 'is', 'a', 'field', 'of', 'artifici', 'intellig', 'that', 'deal', 'with', 'the', 'interact', 'between', 'comput', 'and', 'human', '(', 'natur', ')', 'languag', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Lemmatization**"
      ],
      "metadata": {
        "id": "lj8XWE9GLnbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "QDD5oUlYLwET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "169c9e96-9d3b-4d01-aff6-5f62c182e4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "# create lemmatizer object\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "# lemmatize each token\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(\"Lemmatized tokens:\", lemmatized_tokens)"
      ],
      "metadata": {
        "id": "k1cztfdHLqSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2420d84b-4d1a-49e3-86ad-76057f7ad1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized tokens: ['Natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'deal', 'with', 'the', 'interaction', 'between', 'computer', 'and', 'human', '(', 'natural', ')', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# English pipelines include a rule-based lemmatizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
        "print(lemmatizer.mode)  # 'rule'\n",
        "doc = nlp(\"I was reading the paper.\")\n",
        "print([token.lemma_ for token in doc])\n",
        "# ['I', 'be', 'read', 'the', 'paper', '.']"
      ],
      "metadata": {
        "id": "85S20g_nMFjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "477f71c4-b92a-46de-f165-353f34f2adaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rule\n",
            "['I', 'be', 'read', 'the', 'paper', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Part-of-speech tagging**"
      ],
      "metadata": {
        "id": "-rLAUWu8MHfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "EAD6ubawMad7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce6bb0d5-fb18-4bfd-c3f2-dc39ca248bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "# tag the tokens with their POS tags\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "print(\"Tagged tokens:\", tagged_tokens)\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)"
      ],
      "metadata": {
        "id": "LOgiMLfUMOZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a365635-8ae8-47d6-ca75-b8fc98e9f703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tagged tokens: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('that', 'IN'), ('deals', 'NNS'), ('with', 'IN'), ('the', 'DT'), ('interaction', 'NN'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('human', 'JJ'), ('(', '('), ('natural', 'JJ'), (')', ')'), ('language', 'NN'), ('.', '.')]\n",
            "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
            "is be AUX VBZ aux xx True True\n",
            "looking look VERB VBG ROOT xxxx True False\n",
            "at at ADP IN prep xx True True\n",
            "buying buy VERB VBG pcomp xxxx True False\n",
            "U.K. U.K. PROPN NNP dobj X.X. False False\n",
            "startup startup NOUN NN dep xxxx True False\n",
            "for for ADP IN prep xxx True True\n",
            "$ $ SYM $ quantmod $ False False\n",
            "1 1 NUM CD compound d False False\n",
            "billion billion NUM CD pobj xxxx True False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. Named Entity Recognition**"
      ],
      "metadata": {
        "id": "YCTzDG2jMdBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "id": "Qbm0AZiOMo5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ce7fad-a5f2-49b8-cc22-6b19ffbf360f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language. John Smith works at Google in New York.\"\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "# tag the tokens with their part of speech\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "# identify named entities\n",
        "named_entities = nltk.ne_chunk(tagged_tokens)\n",
        "print(\"Named entities:\", named_entities)"
      ],
      "metadata": {
        "id": "fI3zZ824Mj8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b85d37-554b-4ea6-d9bc-3fb25398a766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named entities: (S\n",
            "  Natural/JJ\n",
            "  language/NN\n",
            "  processing/NN\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  field/NN\n",
            "  of/IN\n",
            "  artificial/JJ\n",
            "  intelligence/NN\n",
            "  that/IN\n",
            "  deals/NNS\n",
            "  with/IN\n",
            "  the/DT\n",
            "  interaction/NN\n",
            "  between/IN\n",
            "  computers/NNS\n",
            "  and/CC\n",
            "  human/JJ\n",
            "  (/(\n",
            "  natural/JJ\n",
            "  )/)\n",
            "  language/NN\n",
            "  ./.\n",
            "  (PERSON John/NNP Smith/NNP)\n",
            "  works/VBZ\n",
            "  at/IN\n",
            "  (ORGANIZATION Google/NNP)\n",
            "  in/IN\n",
            "  (GPE New/NNP York/NNP)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "id": "7L9_28kiMqfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "246f74c4-5a83-46dc-b963-306dbfea1633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple 0 5 ORG\n",
            "U.K. 27 31 GPE\n",
            "$1 billion 44 54 MONEY\n"
          ]
        }
      ]
    }
  ]
}