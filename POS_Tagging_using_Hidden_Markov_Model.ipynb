{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxc8uT9wXry6/X13naUSwW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajarajachozhanVK/RajarajachozhanVK/blob/main/POS_Tagging_using_Hidden_Markov_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aim: To perform the Part of Speech (POS) Tagging using Hidden Markov Models (HMMs) with the Viterbi Algorithm\n",
        "\n",
        "1. Hidden Markov Model(HMM)\n",
        "The Hidden Markov Model is a statistical model that is used to analyze sequential data, such as language, and is particularly useful for tasks like speech recognition, machine translation, and text analysis.\n",
        "1.1 Markovian Assumption\n",
        "Markov models are stochastic models that were created to model processes that occur sequentially. In a Markov process, it is typically assumed that the probability of each event or state depends only on the probability of the preceding event and is independent of its past history. This simplifying assumption is known as the Markovian assumption, and it is a special case of a one-Markov or first-order Markov assumption.\n",
        "1.2 Markov Chain\n",
        "A Markov chain is a mathematical model that represents a process where the system transitions from one state to another. The transition assumes that the probability of moving to the next state is solely dependent on the current state. Please refer to the figure below for an illustration:\n",
        "image.png\n",
        "\n",
        "Fig-1. Markov Chain\n",
        "\n",
        "\n",
        "In the above figure, ‘a’, ‘p’, ‘i’, ‘t’, ‘e’, and ‘h’ represent the states, while the numbers on the edges indicate the probability of transitioning from one state to another. For example, the probability of transitioning from state ‘t’ to states ‘i’, ‘a’, and ‘h’ are 0.3, 0.3, and 0.4, respectively.\n",
        "\n",
        "The start state is a special state that represents the initial state of the process, such as the start of a sentence.\n",
        "\n",
        "Markov processes are commonly used to model sequential data, like text and speech.\n",
        "\n",
        "For instance, if you want to build an application that predicts the next word in a sentence, you can represent each word in a sentence as a state. The transition probabilities can be learned from a corpus and represent the probability of moving from the current word to the next word.\n",
        "\n",
        "For example, the transition probability from the state ‘San’ to ‘Francisco’ will be higher than the probability of transitioning to the state ‘Delhi’.\n",
        "1.3 Hidden Markov Model\n",
        "The Hidden Markov Model (HMM) is a statistical model used to describe systems that follow the Markov property with hidden states. It is widely used in various fields such as natural language processing (NLP), speech recognition, bioinformatics, and more.\n",
        "\n",
        "In an HMM, we have:\n",
        "\n",
        "Hidden states: The true states of the system that are not directly observable.\n",
        "Observations: The visible outputs that are probabilistically dependent on the hidden states.\n",
        "Transition probabilities: The probabilities of transitioning from one hidden state to another.\n",
        "Emission probabilities: The probabilities of observing a specific output given a particular hidden state.\n",
        "Initial probabilities: The probabilities of the system starting in each hidden state.\n",
        "Formalizing the definition, an HMM is a quintuple (S,V,π,A,B), characterized by the following elements (Rabiner and Juang, 1986a):\n",
        "\n",
        "• S = {${S_1,S_2,…,S_N}$} is the set of states, where N is the number of states. The triplet (S,π,A) represents a Markov chain; the states are hidden and we never observe them directly.\n",
        "\n",
        "• V = {${v_1,v_2,…,v_M}$} is the vocabulary, the set of symbols that may be emitted.\n",
        "\n",
        "• π: S→[0.1] = {${π_1,π_2,…,π_M}$} is the initial probability distribution on the states. It gives the probability of starting in each state. We expect that\n",
        "\n",
        "$∑_{(s∈S)}π(S)=∑_{(i=1)}^N {π_i}$ =1\n",
        "\n",
        "• A = $(a_{ij})_{i∈S,j∈S}$ is the transition probability of moving from state $S_{i}$ to state $S_{j}$. We expect that $a_{ij}∈[0,1]$ for each $S_{i}$ and $S_{j}$, and that $∑_{(i∈S)} a_{ij}=1$, for each $S_{j}$.\n",
        "\n",
        "• B = $(b_{ij})_{i∈V,j∈S}$ is the emission probability that symbol v_{i} is seen when we are in state S_{i}.\n",
        "\n",
        "1.4 HMM Types\n",
        "HMM can be trained using a variety of algorithms, including the Viterbi algorithm and the Baum-Welch algorithm.\n",
        "\n",
        "The Viterbi algorithm is a dynamic programming algorithm that finds the most likely sequence of hidden states given a sequence of observable events.\n",
        "\n",
        "The Baum-Welch algorithm is an unsupervised learning algorithm that iteratively adjusts the probabilities of events occurring in each state to fit the data better.\n",
        "\n",
        "1.5 Advantages of the Hidden Markov Model\n",
        "HMM can be trained on large datasets to learn the probabilities of certain events occurring in certain states.\n",
        "For example, HMM can be trained on a corpus of sentences to learn the probability of a verb following a noun or an adjective.\n",
        "\n",
        "1.6 Applications of the Hidden Markov Model\n",
        "Part-of-Speech (POS) Tagging\n",
        "Named Entity Recognition (NER)\n",
        "Speech Recognition\n",
        "Machine Translation\n",
        "1.7 Limitations of the Hidden Markov Model\n",
        "HMM assumes that the probability of an event occurring in a certain state is fixed, which may not always be the case in real-world data.\n",
        "Additionally, HMM is not well-suited for modeling long-term dependencies in language, as it only considers the immediate past.\n",
        "Note: There are alternative models to HMM in NLP, including recurrent neural networks (RNNs) and transformer models like BERT and GPT. These models have shown promising results in a variety of NLP tasks, but they also have their own limitations and challenges.\n",
        "\n",
        "2. Viterbi Algorithm\n",
        "The Viterbi algorithm is a dynamic programming algorithm used to find the most probable sequence of hidden states in a Hidden Markov Model (HMM) given a sequence of observations. Below are the steps of the Viterbi algorithm, explained in detail:\n",
        "\n",
        "2.1 Initialization\n",
        "Initialize the base cases for the time step t=0.\n",
        "For each state s, compute the initial probability as the product of the start probability and the emission probability of the first observation.\n",
        "Store the initial paths for each state.\n",
        "2.3 Recursion\n",
        "For each subsequent observation (from t=1 to T−1), do the following:\n",
        "For each state $s_{j}$ at time t:\n",
        "Compute the probability of the most probable path that ends in $s_{j}$ by considering all possible previous states$s_{i}$ and selecting the one that maximizes the probability of the path ending in $s_{j}$.\n",
        "2.3 Termination\n",
        "After processing all observations, find the state with the highest probability at the final time step.\n",
        "Backtrack through the paths to determine the most probable sequence of states.\n",
        "2.4 Output\n",
        "The most probable sequence of states and the probability of this sequence.\n",
        "\n",
        "3. Weather Prediction Problem using HMM\n",
        "We will use a simple weather prediction example\n",
        "\n",
        "3.1 Step-by-Step Implementation:\n",
        "Define the model parameters.\n",
        "Implement the Viterbi algorithm to find the most probable sequence of states given the observations.\n",
        "hidden-markov-model-.png\n",
        "\n",
        "Fig-2. Weather Prediction using HMM\n",
        "3.1.1 Model Parameters\n",
        "States: Sunny, Rainy\n",
        "Observations: Walk, Shop, Clean\n",
        "Initial Probabilities: Probability of starting in a particular state.\n",
        "P(Sunny)=0.6\n",
        "P(Rainy)=0.4\n",
        "Transition Probabilities: Probability of transitioning from one state to another.\n",
        "P(Sunny∣Sunny)=0.7\n",
        "P(Rainy∣Sunny)=0.3\n",
        "P(Sunny∣Rainy)=0.4\n",
        "P(Rainy∣Rainy)=0.6\n",
        "Emission Probabilities: Probability of an observation given a state.\n",
        "P(Walk∣Sunny)=0.6\n",
        "P(Shop∣Sunny)=0.3\n",
        "P(Clean∣Sunny)=0.1\n",
        "P(Walk∣Rainy)=0.1\n",
        "P(Shop∣Rainy)=0.4\n",
        "P(Clean∣Rainy)=0.5\n",
        "Observations Sequence: ['Walk', 'Shop', 'Clean']\n",
        "3.2 Explanation:\n",
        "The given problem and calculate the output step by step using the Viterbi algorithm for the observation sequence [′𝑊𝑎𝑙𝑘′, 'Shop', 'Clean'].\n",
        "\n",
        "Step 1: Initialization - Set up the initial probabilities for the first observation\n",
        "For t=0:\n",
        "\n",
        "State: 𝑆𝑢𝑛𝑛𝑦\n",
        "\n",
        "𝑉[0][′𝑆𝑢𝑛𝑛𝑦′] = start_probability[′𝑆𝑢𝑛𝑛𝑦′] × emission_probability[′𝑆𝑢𝑛𝑛𝑦′][′𝑊𝑎𝑙𝑘′] = 0.6 × 0.6 = 0.36\n",
        "State: Rainy\n",
        "\n",
        "𝑉[0][′𝑅𝑎𝑖𝑛𝑦′] = start_probability[′𝑅𝑎𝑖𝑛𝑦′] × emission_probability[′𝑅𝑎𝑖𝑛𝑦′][′𝑊𝑎𝑙𝑘′] = 0.4 × 0.1 = 0.04\n",
        "Paths:\n",
        "\n",
        "path[′𝑆𝑢𝑛𝑛𝑦′] = [′𝑆𝑢𝑛𝑛𝑦′]\n",
        "\n",
        "path[′𝑅𝑎𝑖𝑛𝑦′] = [′𝑅𝑎𝑖𝑛𝑦′]\n",
        "\n",
        "Step 2: Recursion - For each time step, compute the maximum probability for each state by considering all possible transitions from the previous states\n",
        "For t=1:\n",
        "\n",
        "State: Sunny\n",
        "\n",
        "𝑉[1][′𝑆𝑢𝑛𝑛𝑦′]=max(𝑉[0][′𝑆𝑢𝑛𝑛𝑦′]×transition_probability[′𝑆𝑢𝑛𝑛𝑦′][′𝑆𝑢𝑛𝑛𝑦′]×emission_probability[′𝑆𝑢𝑛𝑛𝑦′][′Shop′],𝑉[0][′𝑅𝑎𝑖𝑛𝑦′]×transition_probability[′𝑅𝑎𝑖𝑛𝑦′][′𝑆𝑢𝑛𝑛𝑦′]×emission_probability[′𝑆𝑢𝑛𝑛𝑦′][′Shop′])=max(0.36×0.7×0.3,0.04×0.4×0.3)=max(0.0756,0.0048)=0.0756\n",
        "Paths:\n",
        "\n",
        "path[′𝑆𝑢𝑛𝑛𝑦′] = [′𝑆𝑢𝑛𝑛𝑦′]\n",
        "\n",
        "path[′𝑅𝑎𝑖𝑛𝑦′] = [′𝑅𝑎𝑖𝑛𝑦′]\n",
        "\n",
        "new_path[′𝑆𝑢𝑛𝑛𝑦′]=path[′𝑆𝑢𝑛𝑛𝑦′]+[′𝑆𝑢𝑛𝑛𝑦′]=[′𝑆𝑢𝑛𝑛𝑦′,′𝑆𝑢𝑛𝑛𝑦′]\n",
        "\n",
        "State: Rainy\n",
        "\n",
        "𝑉[1][′𝑅𝑎𝑖𝑛𝑦′]=max(𝑉[0][′𝑆𝑢𝑛𝑛𝑦′]×transition_probability[′𝑆𝑢𝑛𝑛𝑦′][′𝑅𝑎𝑖𝑛𝑦′]×emission_probability[′𝑅𝑎𝑖𝑛𝑦′][′Shop′],𝑉[0][′𝑅𝑎𝑖𝑛𝑦′]×transition_probability[′𝑅𝑎𝑖𝑛𝑦′][′𝑅𝑎𝑖𝑛𝑦′]×emission_probability[′𝑅𝑎𝑖𝑛𝑦′][′Shop′])=max(0.36 × 0.3 × 0.4, 0.04 × 0.6 × 0.4)=max(0.0432,0.0096) = 0.0432\n",
        "new_path['𝑅𝑎𝑖𝑛𝑦'] = path[′𝑆𝑢𝑛𝑛𝑦′]+[′𝑅𝑎𝑖𝑛𝑦′] = [′𝑆𝑢𝑛𝑛𝑦′,′𝑅𝑎𝑖𝑛𝑦′]\n",
        "\n",
        "For t=2:\n",
        "\n",
        "State: 𝑆𝑢𝑛𝑛𝑦\n",
        "\n",
        "𝑉[2][′𝑆𝑢𝑛𝑛𝑦′]=max(𝑉[1][′𝑆𝑢𝑛𝑛𝑦′]×transition_probability[′𝑆𝑢𝑛𝑛𝑦′][′𝑆𝑢𝑛𝑛𝑦′]×emission_probability[′𝑆𝑢𝑛𝑛𝑦′][′Clean′],𝑉[1][′𝑅𝑎𝑖𝑛𝑦′]×transition_probability[′𝑅𝑎𝑖𝑛𝑦′][′𝑆𝑢𝑛𝑛𝑦′]×emission_probability[′𝑆𝑢𝑛𝑛𝑦′][′Clean′])=max(0.0756 × 0.7 × 0.1, 0.0432 × 0.4 × 0.1)=max(0.005292,0.001728)=0.005292\n",
        "new_path[′𝑆𝑢𝑛𝑛𝑦′] = path[′𝑆𝑢𝑛𝑛𝑦′]+[′𝑆𝑢𝑛𝑛𝑦′] = [′𝑆𝑢𝑛𝑛𝑦′,′𝑆𝑢𝑛𝑛𝑦′,′𝑆𝑢𝑛𝑛𝑦′]\n",
        "\n",
        "State: 𝑅𝑎𝑖𝑛𝑦\n",
        "\n",
        "𝑉[2][′𝑅𝑎𝑖𝑛𝑦′]=max(𝑉[1][′𝑆𝑢𝑛𝑛𝑦′]×transition_probability[′𝑆𝑢𝑛𝑛𝑦′][′𝑅𝑎𝑖𝑛𝑦′]×emission_probability[′𝑅𝑎𝑖𝑛𝑦′][′Clean′],𝑉[1][′𝑅𝑎𝑖𝑛𝑦′]×transition_probability[′𝑅𝑎𝑖𝑛𝑦′][′𝑅𝑎𝑖𝑛𝑦′]×emission_probability[′𝑅𝑎𝑖𝑛𝑦′][′Clean′])=max(0.0756 × 0.3 × 0.5, 0.0432 × 0.6 × 0.5)=max(0.01134,0.01296)=0.01296\n",
        "new_path[′𝑅𝑎𝑖𝑛𝑦′] = path[′𝑅𝑎𝑖𝑛𝑦′]+[′𝑅𝑎𝑖𝑛𝑦′] = [′𝑆𝑢𝑛𝑛𝑦′,′𝑅𝑎𝑖𝑛𝑦′,′𝑅𝑎𝑖𝑛𝑦′]\n",
        "\n",
        "Step 3: Identify the state with the highest probability at the final time step and backtrack to find the most probable sequence of states. Termination Find the final most probable state and its path:\n",
        "(prob,state) = max((𝑉[2][′𝑆𝑢𝑛𝑛𝑦′],′𝑆𝑢𝑛𝑛𝑦′),(𝑉[2][′𝑅𝑎𝑖𝑛𝑦′],′𝑅𝑎𝑖𝑛𝑦′)) = max((0.005292,′𝑆𝑢𝑛𝑛𝑦′),(0.01296,′𝑅𝑎𝑖𝑛𝑦′)) = (0.01296,′𝑅𝑎𝑖𝑛𝑦′)\n",
        "Thus, the most probable state sequence is [′𝑆𝑢𝑛𝑛𝑦′,′𝑅𝑎𝑖𝑛𝑦′,′𝑅𝑎𝑖𝑛𝑦′] with a probability of 0.01296.\n",
        "\n",
        "\n",
        "Hidden Markov Model Implementation\n",
        "\n",
        "Step 1: Define Model Parameters"
      ],
      "metadata": {
        "id": "CreU8kBqm5gN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lUQ9KIIymPmG"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# States and Observations\n",
        "states = ['Sunny', 'Rainy']\n",
        "observations = ['Walk', 'Shop', 'Clean']\n",
        "# Initial Probabilities\n",
        "start_probability = {'Sunny': 0.6, 'Rainy': 0.4}\n",
        "# Transition Probabilities\n",
        "transition_probability = {\n",
        "    'Sunny': {'Sunny': 0.7, 'Rainy': 0.3},\n",
        "    'Rainy': {'Sunny': 0.4, 'Rainy': 0.6}\n",
        "}\n",
        "# Emission Probabilities\n",
        "emission_probability = {\n",
        "    'Sunny': {'Walk': 0.6, 'Shop': 0.3, 'Clean': 0.1},\n",
        "    'Rainy': {'Walk': 0.1, 'Shop': 0.4, 'Clean': 0.5}\n",
        "}"
      ],
      "metadata": {
        "id": "kBSJa1ccnWQ7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Implement the Viterbi Algorithm\n",
        "\n",
        "The Viterbi algorithm finds the most probable sequence of states given the observations."
      ],
      "metadata": {
        "id": "73aEPAwjnjpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
        "    V = [{}]\n",
        "    path = {}\n",
        "    # Initialize base cases (t == 0)\n",
        "    for state in states:\n",
        "        V[0][state] = start_p[state] * emit_p[state][obs[0]]\n",
        "        path[state] = [state]\n",
        "    # Run Viterbi for t > 0\n",
        "    for t in range(1, len(obs)):\n",
        "        V.append({})\n",
        "        new_path = {}\n",
        "        for curr_state in states:\n",
        "            (prob, state) = max((V[t - 1][prev_state] * trans_p[prev_state][curr_state] * emit_p[curr_state][obs[t]], prev_state) for prev_state in states)\n",
        "            V[t][curr_state] = prob\n",
        "            new_path[curr_state] = path[state] + [curr_state]\n",
        "        # Update path\n",
        "        path = new_path\n",
        "    # Find the final most probable state\n",
        "    (prob, state) = max((V[-1][state], state) for state in states)\n",
        "    return (prob, path[state])\n",
        "# Example observations\n",
        "obs_sequence = ['Walk', 'Shop', 'Clean']\n",
        "# Running the Viterbi algorithm\n",
        "prob, state_sequence = viterbi(obs_sequence, states, start_probability, transition_probability, emission_probability)\n",
        "print(\"Observation Sequence:\", obs_sequence)\n",
        "print(\"Most probable state sequence:\", state_sequence)\n",
        "print(\"Probability of the state sequence:\", prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weVkPl-ung7N",
        "outputId": "aecb142a-22b7-42c0-8a9f-773bba87d9b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation Sequence: ['Walk', 'Shop', 'Clean']\n",
            "Most probable state sequence: ['Sunny', 'Rainy', 'Rainy']\n",
            "Probability of the state sequence: 0.012960000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Part of Speech (POS) Tagging using HMMs with the Viterbi Algorithm\n",
        "Overview of HMMs for POS Tagging An HMM is defined by:\n",
        "\n",
        "States: Correspond to the POS tags.\n",
        "Observations: Correspond to the words in the sentence.\n",
        "Transition Probabilities $(𝑃(𝑇_{𝑖}∣𝑇_{𝑖−1)})$: Probability of transitioning from one tag to another.\n",
        "Emission Probabilities $(𝑃(𝑊_{𝑖}∣𝑇_{𝑖}))$: Probability of a word given a tag.\n",
        "Initial Probabilities $(𝑃(𝑇_{1}))$: Probability of starting with a particular tag.\n",
        "5.1 Steps for POS Tagging with HMM\n",
        "1. Training:\n",
        "\n",
        "Estimate Initial Probabilities: $𝑃(𝑇_{1})$ is estimated as the frequency of each tag at the start of sentences in the training corpus.\n",
        "Estimate Transition Probabilities: $𝑃(𝑇_{𝑖}∣𝑇_{𝑖−1})$ is estimated from the frequency of tag pairs in the training corpus.\n",
        "Estimate Emission Probabilities: $𝑃(𝑊_{𝑖}∣𝑇_{𝑖})$ is estimated from the frequency of words given tags in the training corpus.\n",
        "2. Decoding:\n",
        "\n",
        "Given a sequence of words (observations), determine the most likely sequence of tags (states) using the Viterbi algorithm.\n",
        "5.2 Viterbi Algorithm\n",
        "The Viterbi algorithm is a dynamic programming algorithm used to find the most probable sequence of hidden states (tags) in a Hidden Markov Model (HMM) given a sequence of observations (words). Here's a step-by-step guide to implementing the Viterbi algorithm for our weather prediction example in Python:\n",
        "\n",
        "5.3 Example\n",
        "Assume we have a simplified tag set: Noun (N), Verb (V), Adjective (Adj) and a small vocabulary.\n",
        "\n",
        "5.3.1 Training Data\n",
        "Training Sentences:\n",
        "\n",
        "\"fish swim\"\n",
        "\"birds fly\"\n",
        "\"fish fish\"\n",
        "\"fish are good\"\n",
        "5.3.2 Tagging\n",
        "fish/N swim/V\n",
        "birds/N fly/V\n",
        "fish/N fish/V\n",
        "fish/N are/V good/Adj\n",
        "5.3.3 Probabilities Estimation\n",
        "Initial Probabilities:\n",
        "P(N) = 1.0 (all sentences start with a Noun)\n",
        "Transition Probabilities:\n",
        "P(N | N) = 0.25 (1 out of 4 times a Noun is followed by a Noun)\n",
        "P(V | N) = 0.75 (3 out of 4 times a Noun is followed by a Verb)\n",
        "P(Adj | V) = 1.0 (1 out of 3 times a Verb is followed by an Adjective)\n",
        "P(V | V) = 0.67 (2 out of 3 times a Verb is followed by another Verb)\n",
        "Emission Probabilities:\n",
        "P(fish | N) = 0.67 (2 out of 3 times \"fish\" is a Noun)\n",
        "P(fish | V) = 0.33 (1 out of 3 times \"fish\" is a Verb)\n",
        "P(swim | V) = 0.33\n",
        "P(birds | N) = 1.0\n",
        "P(fly | V) = 0.33\n",
        "P(are | V) = 0.33\n",
        "P(good | Adj) = 1.0\n",
        "5.3.4 Viterbi Algorithm Application\n",
        "For a new sentence \"fish are swim\":\n",
        "1. Initialization:\n",
        "t = 1\n",
        "v(N, 1) = P(N) * P(fish | N) = 1.0 * 0.67\n",
        "v(V, 1) = 0 (no initial probability of starting with V)\n",
        "v(Adj, 1) = 0 (no initial probability of starting with Adj)\n",
        "2. Recursion:\n",
        "2.1) t = 2 (for word \"are\")\n",
        "v(N, 2) = max(v(N, 1) * P(N | N), v(V, 1) * P(N | V)) * P(are | N) = 0\n",
        "v(V, 2) = max(v(N, 1) * P(V | N), v(V, 1) * P(V | V)) * P(are | V) = 0.67 * 0.75 * 0.33\n",
        "v(Adj, 2) = 0 (no transition to Adj directly)\n",
        "2.2) t = 3 (for word \"swim\")\n",
        "v(N, 3) = 0 (no probability for N)\n",
        "v(V, 3) = max(v(N, 2) * P(V | N), v(V, 2) * P(V | V)) * P(swim | V) = 0.67 * 0.33\n",
        "v(Adj, 3) = max(v(V, 2) * P(Adj | V), v(Adj, 2) * P(Adj | Adj)) * P(swim | Adj) = 0\n",
        "3. Termination:\n",
        "The most probable final state:\n",
        "\n",
        "Tag for \"swim\" is V\n",
        "Thus, the sequence of tags for \"fish are swim\" is N V V.\n",
        "\n",
        "Part of Speech (POS) tagging Implementation"
      ],
      "metadata": {
        "id": "UztiTqgvn1hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training sentences and their corresponding tags\n",
        "train_sentences = [\n",
        "    [\"fish\", \"swim\"],\n",
        "    [\"birds\", \"fly\"],\n",
        "    [\"fish\", \"fish\"],\n",
        "    [\"fish\", \"are\", \"good\"]\n",
        "]\n",
        "train_tags = [\n",
        "    [\"N\", \"V\"],\n",
        "    [\"N\", \"V\"],\n",
        "    [\"N\", \"V\"],\n",
        "    [\"N\", \"V\", \"Adj\"]\n",
        "]\n",
        "# Vocabulary and tag set\n",
        "vocab = set(word for sentence in train_sentences for word in sentence)\n",
        "tagset = set(tag for tags in train_tags for tag in tags)"
      ],
      "metadata": {
        "id": "GNzjfJUIn4hm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Calculate Probabilities\n",
        "\n",
        "Next, let's estimate the initial, transition, and emission probabilities"
      ],
      "metadata": {
        "id": "-kToGPvFoDt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Calculate initial probabilities\n",
        "initial_probs = Counter(tag[0] for tag in train_tags)\n",
        "total_sentences = len(train_sentences)\n",
        "for tag in initial_probs:\n",
        "    initial_probs[tag] /= total_sentences\n",
        "# Calculate transition probabilities\n",
        "transitions = defaultdict(Counter)\n",
        "for tags in train_tags:\n",
        "    for i in range(len(tags) - 1):\n",
        "        transitions[tags[i]][tags[i+1]] += 1\n",
        "transition_probs = defaultdict(dict)\n",
        "for prev_tag in transitions:\n",
        "    total = sum(transitions[prev_tag].values())\n",
        "    for next_tag in transitions[prev_tag]:\n",
        "        transition_probs[prev_tag][next_tag] = transitions[prev_tag][next_tag] / total\n",
        "# Calculate emission probabilities\n",
        "emissions = defaultdict(Counter)\n",
        "for sentence, tags in zip(train_sentences, train_tags):\n",
        "    for word, tag in zip(sentence, tags):\n",
        "        emissions[tag][word] += 1\n",
        "emission_probs = defaultdict(dict)\n",
        "for tag in emissions:\n",
        "    total = sum(emissions[tag].values())\n",
        "    for word in emissions[tag]:\n",
        "        emission_probs[tag][word] = emissions[tag][word] / total"
      ],
      "metadata": {
        "id": "LAxf_YkloEzm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Viterbi Algorithm\n",
        "\n",
        "Now, let's implement the Viterbi algorithm to predict the POS tags for a given sentence"
      ],
      "metadata": {
        "id": "bnhRSaayoP04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi(sentence, initial_probs, transition_probs, emission_probs, tagset):\n",
        "    V = [{}]\n",
        "    path = {}\n",
        "    # Initialize base cases (t == 0)\n",
        "    for tag in tagset:\n",
        "        V[0][tag] = initial_probs.get(tag, 0) * emission_probs[tag].get(sentence[0], 0)\n",
        "        path[tag] = [tag]\n",
        "    # Run Viterbi for t > 0\n",
        "    for t in range(1, len(sentence)):\n",
        "        V.append({})\n",
        "        newpath = {}\n",
        "        for tag in tagset:\n",
        "            (prob, state) = max((V[t-1][y0] * transition_probs[y0].get(tag, 0) * emission_probs[tag].get(sentence[t], 0), y0) for y0 in tagset)\n",
        "            V[t][tag] = prob\n",
        "            newpath[tag] = path[state] + [tag]\n",
        "        path = newpath\n",
        "    # Return the most likely sequence\n",
        "    n = len(sentence) - 1\n",
        "    (prob, state) = max((V[n][tag], tag) for tag in tagset)\n",
        "    return path[state]"
      ],
      "metadata": {
        "id": "QPp4Y3KDoRxv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Test the Viterbi Algorithm\n",
        "\n",
        "Let's test the Viterbi algorithm with a sample sentence"
      ],
      "metadata": {
        "id": "xr-pBR24oYHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = [\"fish\", \"are\", \"swim\"]\n",
        "predicted_tags = viterbi(sentence, initial_probs, transition_probs, emission_probs, tagset)\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"Predicted Tags:\", predicted_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1R2lddDohAH",
        "outputId": "696e74f1-eea5-476c-aced-8f9d63924777"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: ['fish', 'are', 'swim']\n",
            "Predicted Tags: ['N', 'V', 'V']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "\n",
        "POS tagging with HMMs is powerful due to its probabilistic nature and the ability to model sequences. The Viterbi algorithm is essential in decoding the most probable tag sequence efficiently."
      ],
      "metadata": {
        "id": "2PfeoOrgolxR"
      }
    }
  ]
}